# -*- coding: utf-8 -*-
"""Capsnet_spectrogram_added .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XWcfu-dLOMIzNyn_oeaDHU5bXQDFrNzQ
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install nina-helper
# %pip install tensorflow_addons
# %pip install pyts

from functools import reduce
import joblib
import matplotlib.pyplot as plt
from nina_helper import *
import numpy as np
import os
import pandas as pd
from pywt import wavedec

import scipy as sp
from scipy import signal, interp
from scipy.fft import fft, ifft
from scipy.io import loadmat
from scipy.stats import entropy

import seaborn as sns

from sklearn import metrics
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, balanced_accuracy_score, classification_report
from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_validate
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC

from tensorflow import keras
from tensorflow.keras import layers, models, optimizers, initializers
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.layers import Conv1D,Conv2D, Add, MaxPool1D, MaxPooling2D
from tensorflow.keras.layers import Dense, Activation, Flatten, concatenate, Input, Dropout
from tensorflow.keras.layers import LSTM, Bidirectional,BatchNormalization,PReLU,ReLU,Reshape
from tensorflow.keras.models import Sequential, Model, load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical
from tensorflow_addons.layers import WeightNormalization

import tensorflow.keras.backend as K

import tensorflow as tf
config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True
sess = tf.compat.v1.InteractiveSession(config=config)


def Statistics(data):
  # Classification Report
  report = classification_report(data['actual labels'],data['predicted labels'],output_dict=True)
  report = pd.DataFrame(report).T
  # Confusion matrix
  print("Confusion matrix is shown below")
  c_matrix=confusion_matrix(data['actual labels'],data['predicted labels'])
  FP = c_matrix.sum(axis=0) - np.diag(c_matrix)
  FN = c_matrix.sum(axis=1) - np.diag(c_matrix)
  TP = np.diag(c_matrix)
  TN = c_matrix.sum() - (FP + FN + TP)
  FP = FP.astype(float)
  FN = FN.astype(float)
  TP = TP.astype(float)
  TN = TN.astype(float)
  # Sensitivity or positive recall
  TPR = TP/(TP+FN)
  # Specificity or true negative rate or negative recall
  TNR = TN/(TN+FP)
  accuracy = accuracy_score(data['actual labels'],data['predicted labels'])
  TNR = np.append(TNR,[accuracy, TNR.mean(), ((TNR * report['support'][:17]).sum())/(report['support'][:17].sum())])
  report['specificity'] = TNR # adding new column specificity
  plt.figure(figsize=(40,20))
  norm_c_matrix = c_matrix.astype('float') / c_matrix.sum(axis=1)[:, np.newaxis]
  sns.heatmap(norm_c_matrix, annot=True,cmap='Blues', fmt='.2f')
  #sns.heatmap(c_matrix, annot=True,cmap='Blues', fmt='g')
  plt.xlabel('Predicted')
  plt.ylabel('Truth')
  plt.savefig("CapsNet_confusion_mat_{}{}.jpg".format(int(window_len/2),'ms'))
  print("Balanced_accuracy:{}".format(balanced_accuracy_score(data['actual labels'],data['predicted labels'])))
  print("Classification Report is shown below")
  return report, c_matrix, norm_c_matrix

class Length(layers.Layer):
    """
    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss
    inputs: shape=[dim_1, ..., dim_{n-1}, dim_n]
    output: shape=[dim_1, ..., dim_{n-1}]
    """
    def call(self, inputs, **kwargs):
        return K.sqrt(K.sum(K.square(inputs), -1))

    def compute_output_shape(self, input_shape):
        return input_shape[:-1]

class Mask(layers.Layer):
    """
    Mask a Tensor with shape=[None, d1, d2] by the max value in axis=1.
    Output shape: [None, d2]
    """
    def call(self, inputs, **kwargs):
        # use true label to select target capsule, shape=[batch_size, num_capsule]
        if type(inputs) is list:  # true label is provided with shape = [batch_size, n_classes], i.e. one-hot code.
            assert len(inputs) == 2
            inputs, mask = inputs
        else:  # if no true label, mask by the max length of vectors of capsules
            x = inputs
            # Enlarge the range of values in x to make max(new_x)=1 and others < 0
            x = (x - K.max(x, 1, True)) / K.epsilon() + 1
            mask = K.clip(x, 0, 1)  # the max value in x clipped to 1 and other to 0

        # masked inputs, shape = [batch_size, dim_vector]
        inputs_masked = K.batch_dot(inputs, mask, [1, 1])
        return inputs_masked

    def compute_output_shape(self, input_shape):
        if type(input_shape[0]) is tuple:  # true label provided
            return tuple([None, input_shape[0][-1]])
        else:
            return tuple([None, input_shape[-1]])



def squash(vectors, axis=-1):
    """
    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0
    :param vectors: some vectors to be squashed, N-dim tensor
    :param axis: the axis to squash
    :return: a Tensor with same shape as input vectors
    """
    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)
    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm)
    return scale * vectors

class CapsuleLayer(layers.Layer):

    def __init__(self, num_capsule, dim_capsule, routings=3,
                 kernel_initializer='glorot_uniform',
                 **kwargs):
        super(CapsuleLayer, self).__init__(**kwargs)
        self.num_capsule = num_capsule
        self.dim_capsule = dim_capsule
        self.routings = routings
        #self.kernel_initializer = initializers.get(kernel_initializer)
        self.kernel_initializer = initializers.random_uniform(-1, 1)

    def build(self, input_shape):
        assert len(input_shape) >= 3, "The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]"
        self.input_num_capsule = input_shape[1]
        self.input_dim_capsule = input_shape[2]

        # Transform matrix
        self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,
                                        self.dim_capsule, self.input_dim_capsule],
                                 initializer=self.kernel_initializer,
                                 name='W')

        self.built = True

    def call(self, inputs, training=None):
        # Expand the input in axis=1, tile in that axis to num_capsule, and
        # expands another axis at the end to prepare the multiplication with W.
        #  inputs.shape=[None, input_num_capsule, input_dim_capsule]
        #  inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]
        #  inputs_tiled.shape=[None, num_capsule, input_num_capsule,
        #                            input_dim_capsule, 1]
        inputs_expand = tf.expand_dims(inputs, 1)
        inputs_tiled  = tf.tile(inputs_expand, [1, self.num_capsule, 1, 1])
        inputs_tiled  = tf.expand_dims(inputs_tiled, 4)

        # Compute `W * inputs` by scanning inputs_tiled on dimension 0 (map_fn).
        # - Use matmul (without transposing any element). Note the order!
        # Thus:
        #  x.shape=[num_capsule, input_num_capsule, input_dim_capsule, 1]
        #  W.shape=[num_capsule, input_num_capsule, dim_capsule,input_dim_capsule]
        # Regard the first two dimensions as `batch` dimension,
        # then matmul: [dim_capsule, input_dim_capsule] x [input_dim_capsule, 1]->
        #              [dim_capsule, 1].
        #  inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule, 1]

        inputs_hat = tf.map_fn(lambda x: tf.matmul(self.W, x), elems=inputs_tiled)

        # Begin: Routing algorithm ----------------------------------------------#
        # The prior for coupling coefficient, initialized as zeros.
        #  b.shape = [None, self.num_capsule, self.input_num_capsule, 1, 1].
        b = tf.zeros(shape=[tf.shape(inputs_hat)[0], self.num_capsule,
                            self.input_num_capsule, 1, 1])

        assert self.routings > 0, 'The routings should be > 0.'
        for i in range(self.routings):
          # Apply softmax to the axis with `num_capsule`
          #  c.shape=[batch_size, num_capsule, input_num_capsule, 1, 1]
          c = layers.Softmax(axis=1)(b)

          # Compute the weighted sum of all the predicted output vectors.
          #  c.shape =  [batch_size, num_capsule, input_num_capsule, 1, 1]
          #  inputs_hat.shape=[None, num_capsule, input_num_capsule,dim_capsule,1]
          # The function `multiply` will broadcast axis=3 in c to dim_capsule.
          #  outputs.shape=[None, num_capsule, input_num_capsule, dim_capsule, 1]
          # Then sum along the input_num_capsule
          #  outputs.shape=[None, num_capsule, 1, dim_capsule, 1]
          # Then apply squash along the dim_capsule
          outputs = tf.multiply(c, inputs_hat)
          outputs = tf.reduce_sum(outputs, axis=2, keepdims=True)
          outputs = squash(outputs, axis=-2)  # [None, 10, 1, 16, 1]

          if i < self.routings - 1:
            # Update the prior b.
            #  outputs.shape =  [None, num_capsule, 1, dim_capsule, 1]
            #  inputs_hat.shape=[None,num_capsule,input_num_capsule,dim_capsule,1]
            # Multiply the outputs with the weighted_inputs (inputs_hat) and add
            # it to the prior b.
            outputs_tiled = tf.tile(outputs, [1, 1, self.input_num_capsule, 1, 1])
            agreement = tf.matmul(inputs_hat, outputs_tiled, transpose_a=True)
            b = tf.add(b, agreement)

        # End: Routing algorithm ------------------------------------------------#
        # Squeeze the outputs to remove useless axis:
        #  From  --> outputs.shape=[None, num_capsule, 1, dim_capsule, 1]
        #  To    --> outputs.shape=[None, num_capsule,    dim_capsule]
        outputs = tf.squeeze(outputs, [2, 4])
        return outputs

    def compute_output_shape(self, input_shape):
        return tuple([None, self.num_capsule, self.dim_capsule])

    def get_config(self):
        config = {
            'num_capsule': self.num_capsule,
            'dim_capsule': self.dim_capsule,
            'routings': self.routings
        }
        base_config = super(CapsuleLayer, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


def PrimaryCap(inputs, dim_capsule, n_channels, kernel_size, strides, padding):
    """
    Apply Conv2D `n_channels` times and concatenate all capsules
    :param inputs: 4D tensor, shape=[None, width, height, channels]
    :param dim_capsule: the dim of the output vector of capsule
    :param n_channels: the number of types of capsules
    :return: output tensor, shape=[None, num_capsule, dim_capsule]
    """
    output = layers.Conv2D(filters=dim_capsule*n_channels, kernel_size=kernel_size, strides=strides, padding=padding,
                           name='primarycap_conv2d')(inputs)
    # removed : ,use_bias=True, kernel_regularizer =tf.keras.regularizers.l2()
    # this one was not removed for the most recent only

    outputs = layers.Reshape(target_shape=[-1, dim_capsule], name='primarycap_reshape')(output)
    return layers.Lambda(squash, name='primarycap_squash')(outputs)

def CapsNet(input_shape, n_class, num_routing):
    """
    A Capsule Network on MNIST.
    :param input_shape: data shape, 4d, [None, width, height, channels]
    :param n_class: number of classes
    :param num_routing: number of routing iterations
    :return: A Keras Model with 2 inputs and 2 outputs
    """
    x = layers.Input(shape=input_shape)
    # Layer 1: Just a conventional Conv2D layer
    conv1 = layers.Conv2D(filters=256, kernel_size=6, strides=1, padding='valid', activation='relu', name='conv1')(x)
    # removed : ,use_bias=True, kernel_regularizer =tf.keras.regularizers.l2()

    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_vector]
    primarycaps = PrimaryCap(conv1, dim_capsule=8, n_channels=32, kernel_size=6, strides=2, padding='valid')

    # Layer 3: Capsule layer. Routing algorithm works here.
    digitcaps = CapsuleLayer(num_capsule=n_class, dim_capsule=16, name='digitcaps')(primarycaps)

    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.
    # If using tensorflow, this will not be necessary. :)
    out_caps = Length(name='out_caps')(digitcaps)

    # Decoder network.
    y = layers.Input(shape=(n_class,))
    masked = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer.
    
    # added in another layer so it is not direct to 7200 sig a.f.
    x_recon = layers.Dense(512, activation='relu')(masked)
    x_recon = layers.Dense(1024, activation='relu')(x_recon)  
  
    x_recon = layers.Dense(7200, activation='sigmoid')(x_recon)
    x_recon = layers.Reshape(target_shape=[600, 12, 1], name='out_recon')(x_recon)

    # two-input-two-output keras Model

    return models.Model([x, y], [out_caps, x_recon])

def margin_loss(y_true, y_pred):
    """
    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.
    :param y_true: [None, n_classes]
    :param y_pred: [None, num_capsule]
    :return: a scalar loss value.
    """
    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \
        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))

    return K.mean(K.sum(L, 1))

#for data augumentation
def train_generator(x, y, batch_size, shift_fraction=0.):
        train_datagen = ImageDataGenerator(width_shift_range=shift_fraction,
                                           height_shift_range=shift_fraction)

	# removed ,shear_range=0.2,zoom_range=0.2, vertical_flip

        generator = train_datagen.flow(x, y, batch_size=batch_size)
        while 1:
            x_batch, y_batch = generator.next()
            yield ([x_batch, y_batch], [y_batch, x_batch])

def train_model(model, X_train_wind, y_train_wind, X_test_wind, y_test_wind, save_to, batch_size = 64, learning_rate=0.0001, epochs = 2, epoch_size_frac=1.0):

        opt_adam = keras.optimizers.Adam(learning_rate=0.0001)
        model.compile(optimizer=opt_adam,
                  loss=[margin_loss, 'mse'],
                  loss_weights=[1., 0.0005],
                  metrics={'out_caps': 'accuracy'})

        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)
        mc = ModelCheckpoint(save_to + '_best_model.h5', monitor='val_out_caps_accuracy', mode='max', verbose=1, save_best_only=True)
        #lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: learning_rate * np.exp(-epoch / 10.), verbose=1)
        lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch, lr: lr*1.2 if epoch%10==0 else lr * np.exp(-epoch / 400.), verbose=1)
# Train_Generator initially had bs = 512
        history = model.fit(train_generator(X_train_wind, y_train_wind, 64, 0.1),
                            steps_per_epoch=int(epoch_size_frac*y_train_wind.shape[0] / 64),
                            epochs=epochs,
			    batch_size = batch_size,
                            validation_data=([X_test_wind, y_test_wind], [y_test_wind, X_test_wind]),
                            callbacks=[mc,lr_schedule])

        saved_model = load_model(save_to + '_best_model.h5', custom_objects={'CapsuleLayer': CapsuleLayer, 'Mask': Mask, 'Length': Length, 'margin_loss': margin_loss})
	
        """ evaluate the model
        _, train_acc = saved_model.evaluate(X_train_wind, y_train_wind, verbose=0)
        _, test_acc = saved_model.evaluate(X_test_wind, y_test_wind, verbose=0)
        print('Train: %.3f, Test: %.3f' % (train_acc, test_acc)) """

        return history,saved_model

def get_categorical(y):
    return pd.get_dummies(pd.Series(y)).values

def Win2Spec(data):
  # data shape must be (samples, windows, channels)
  spec_list = []
  spec_mat_list = []
  for channel in range(data.shape[2]):
    for sample in range(data.shape[0]):
      _,_,Sxx =  signal.spectrogram(data[sample,:,channel].T, nperseg = 600, fs=2000, window='hamming', return_onesided=False)
      spec_list.append(np.squeeze(Sxx))

    spec_mat = np.array(spec_list)
    # Scale the spectrograms for each channel based on the max reading for its respective channel
    spec_mat = spec_mat/spec_mat.max()
    spec_mat_list.append(spec_mat)
    spec_list = []

  spec_data = np.dstack(spec_mat_list)

  return spec_data


################### Load Windowed Data + Prepare for Input #####################
# X_train = np.load("/content/drive/MyDrive/Smart Wearable Devices: Summer Research/Software Team/40s_gesture_cluster_data/1_of_each_cluster/gestures_1_4_7_10_13/1_of_each_cluster_train_matrix.npy",mmap_mode='r')
# X_test = np.load("/content/drive/MyDrive/Smart Wearable Devices: Summer Research/Software Team/40s_gesture_cluster_data/1_of_each_cluster/gestures_1_4_7_10_13/1_of_each_cluster_test_matrix.npy",mmap_mode='r')
# y_train = np.load("/content/drive/MyDrive/Smart Wearable Devices: Summer Research/Software Team/40s_gesture_cluster_data/1_of_each_cluster/gestures_1_4_7_10_13/1_of_each_cluster_train_labels.npy",mmap_mode='r')
# y_test = np.load("/content/drive/MyDrive/Smart Wearable Devices: Summer Research/Software Team/40s_gesture_cluster_data/1_of_each_cluster/gestures_1_4_7_10_13/1_of_each_cluster_test_labels.npy",mmap_mode='r')

# Decide window length
window_len = 600  # Equivalent window length since sampled 20 times faster
window_inc = 20

# Load 300ms Data
data_path = "/scratch/et1799/Smart_Wearable_Devices/Classifiers/300ms/CNN/"

X_train = np.load(os.path.normpath(data_path + '40s_cnn_train_matrix_300ms.npy'))
X_test = np.load(os.path.normpath(data_path + '40s_cnn_test_matrix_300ms.npy'))
y_train = np.load(os.path.normpath(data_path + '40s_cnn_train_labels_300ms.npy'))
y_test = np.load(os.path.normpath(data_path + '40s_cnn_test_labels_300ms.npy'))

print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)

#y_train = np.argmax(y_train, axis=1)
#y_test = np.argmax(y_test, axis=1)
#print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)

#y_train = pd.get_dummies(y_train)
#y_test = pd.get_dummies(y_test)

# y_train = get_categorical(y_train)
# y_test = get_categorical(y_test)

y_train = y_train.astype('float32')
y_test = y_test.astype('float32')

print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

Sxx_train = Win2Spec(X_train)
Sxx_test = Win2Spec(X_test)

Sxx_train = Sxx_train.reshape((-1,600,12,1))
Sxx_test = Sxx_test.reshape((-1,600,12,1))

print(Sxx_train.shape , Sxx_test.shape, y_train.shape, y_test.shape)

######################### Build+Train Model ####################################
model = CapsNet(input_shape=(600, 12, 1), n_class = 17,num_routing=3)
model.summary()

# Hyper-parameters
batch_size = 128 #2048
epochs = 100
# epoch_size_frac=1
initial_learning_rate = 1e-6
# DO = "N/A"
loss = 'categorical_crossentropy'
optimizer = 'opt_adam'

histories, model = train_model(model, Sxx_train, y_train, Sxx_test , y_test,
                               save_to= 'temp_CapsNet+spectrogram_{}epochs_{}lr_{}bs'.format(epochs, initial_learning_rate, batch_size),
                               batch_size=batch_size, learning_rate=initial_learning_rate, epochs = epochs)

print("batch_size, epochs, initial_learning_rate, loss, optimizer:")
print(batch_size, epochs, initial_learning_rate, loss, optimizer)
################################ PLOTS #########################################
# summarize history for lr
plt.clf
plt.plot(histories.history['lr'])
plt.title('learning rate')
plt.ylabel('lr')
plt.xlabel('epoch')
plt.savefig("CapsNet+spectrogram_lr_{}ms_{}bs_{}epochs.jpg".format(int(window_len/2), batch_size, epochs))

# summarize history for categorical_accuracy
plt.clf
plt.plot(histories.history['out_caps_accuracy'])
plt.plot(histories.history['val_out_caps_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.savefig("CapsNet+spectrogram_categorical_accuracy_{}ms_{}bs_{}epochs.jpg".format(int(window_len/2), batch_size, epochs))

# summarize history for loss
plt.clf
plt.plot(histories.history['out_caps_loss'])
plt.plot(histories.history['val_out_caps_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.savefig("CapsNet+spectrogram_loss_{}ms_{}bs_{}epochs.jpg".format(int(window_len/2), batch_size, epochs))

print(histories.history)

####################### Confusion Matrix + Report ##############################
y_test = np.array(y_test)
y_test = np.argmax(y_test,axis=1)
y_pred, _  = model.predict([Sxx_test, np.zeros((X_test.shape[0],17))], batch_size=batch_size, verbose=1)
y_pred=np.argmax(y_pred, axis=1)
print("y_test, y_pred:", y_test.shape, y_pred.shape)

data = []
data.append(y_test)
data.append(y_pred)
data = np.vstack(data).T
data_df = pd.DataFrame(data, columns = ['actual labels', 'predicted labels'])
report, c_matrix, norm_c_matrix = Statistics(data_df)

# Save 3 outputs from statistics
joblib.dump(report,'CapsNet+spectrogram_report_{}ms'.format(int(window_len/2)))
joblib.dump(c_matrix,'CapsNet+spectrogram_c_matrix_{}ms'.format(int(window_len/2)))
joblib.dump(norm_c_matrix,'CapsNet+spectrogram_norm_c_matrix_{}ms'.format(int(window_len/2)))

